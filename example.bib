
@article{Tucci2010,
	abstract = {Background: Hearing loss (HL) and deafness are global issues that affect at least 278 million people worldwide. Two thirds of the people who have HL worldwide live in developing countries. Importantly, it is estimated that 50{\%} of this HL can be prevented. In developing countries, funding for prevention, early detection, and rehabilitative programs is severely limited, and therefore, agencies must compete against priorities to treat life-threatening, pandemic diseases such as human immunodeficiency virus, malaria, and tuberculosis. Delays in diagnosis are common, and social attitudes, local customs, and cultural bias are contributing factors. Objective: The purpose of this review is to gain an understanding of the prevalence of HL in the developing world and to focus attention on the growing need for both prevention and effective treatment programs. A second goal is to use this information to suggest priorities and approaches to address these problems worldwide. Data Sources: The data were compiled from a review of the literature on the global impacts of hearing impairment and recently published reports on the prevalence and cause of hearing impairment in developing nations. Conclusion:: The high prevalence of HL in the developing world is due to a variety of factors, including lack of widespread comprehensive immunization programs and other medical care, and inadequate funds for intervention once HL is identified. International organizations, governments, and nongovernment organizations have many opportunities to prevent and treat HL through cost-effective means. {\textcopyright} 2009, Otology {\&} Neurotology, Inc.},
	author = {Tucci, Debara L. and Merson, Michael H. and Wilson, Blake S.},
	doi = {10.1097/MAO.0b013e3181c0eaec},
	isbn = {15317129 (ISSN)},
	issn = {15317129},
	journal = {Otology and Neurotology},
	keywords = {Deafness,Global health hearing,Hearing impairment,Hearing loss},
	mendeley-groups = {Subtitle Project},
	pmid = {20050266},
	title = {{A summary of the literature on global hearing impairment: Current status and priorities for action}},
	year = {2010}
}
@misc{subtitledshows, title={Subtitled Cinema Showings in London}, url={http://yourlocalcinema.com/london.central.html}, journal={Subtitled Shows}, publisher={YourLocalCinema}} 

@misc{dolbysubs, title={Dolby CaptiView}, url={https://www.dolby.com/us/en/professional/cinema/products/captiview.html}, journal={Dolby Subtitle Technologies}}

@misc{hoyts, title={HOYTS Cinemas}, url={https://www.hoyts.co.nz/experiences/cc}, journal={Hoyts}}

@misc{Sabater_2017, title={Automatic Subtitle Synchronization â€“ Machine Learnings}, url={https://machinelearnings.co/automatic-subtitle-synchronization-e188a9275617}, journal={Machine Learnings}, publisher={Machine Learnings}, author={Sabater, Alberto}, year={2017}}

@article{Ortega2009,
	abstract = {Subtitling of live broadcast news is a very important application to meet the needs of deaf and hard of hearing people. However, live subtitling is a high cost operation in terms of qualification human resources and therefore, money if high precision is desired. Automatic Speech Recognition researchers can help to perform this task saving both time and money developing systems that deliver subtitles fully synchronized with speech without human assistance. In this paper we present a real-time system for automatic subtitling of live broadcast news in Spanish based on the News Redaction Computer texts and an Automatic Speech Recognition engine to provide precise temporal alignment of speech to text scripts with negligible latency. The presented system is working satisfactory on the Aragonese Public Television from June 2008 without human assistance. Copyright {\textcopyright} 2009 ISCA.},
	author = {Ortega, Alfonso and Garcia, Jose Enrique and Miguel, Antonio and Lleida, Eduardo},
	file = {:Users/joshfenech/Library/Application Support/Mendeley Desktop/Downloaded/Ortega et al. - 2009 - Real-time live broadcast news subtitling system for Spanish.pdf:pdf},
	issn = {19909772},
	journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
	keywords = {Broadcast news,Speech recognition,Subtitling},
	mendeley-groups = {Subtitle Project},
	pages = {2095--2098},
	title = {{Real-time live broadcast news subtitling system for Spanish}},
	url = {http://vivolab.es/demos/subtitle/subtitling.pdf},
	year = {2009}
}
@article{Campbell1996,
	abstract = {Focussing on the prosodic labelling of Japanese as an example, the paper describes the application of speech synthesis technology in a variety of speech processing tasks. It discusses first the use of synthesised utterances in the forced alignment and segmentation of a speech corpus, then the use of generated prosodic contours to determine the prosodic phrasing of an utterance, and finally the comparison with speech resynthesised using the prosodic transcription of the original utterance in order to check the transcription. It closes with an analysis of results from an auto transcription of Japanese ToBI, and discusses some limitations of the proposed J-ToBI system},
	author = {Campbell, N},
	doi = {10.1109/ICSLP.1996.607292},
	file = {:Users/joshfenech/Documents/Linux Documents Backup May/Documents/Shared Documents/MLDM/Project/Papers/Tobi sync TTS.pdf:pdf},
	isbn = {0-7803-3555-4},
	journal = {ICSLP 96. Fourth International Congress on Conference on Language Processing Proceedings},
	mendeley-groups = {Subtitle Project},
	pages = {2399 -- 2402},
	title = {{Autolabelling Japanese ToBI}},
	url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=607292},
	volume = {4},
	year = {1996}
}
@article{mfcc_steps,
	author    = {D. S. Pavan Kumar},
	title     = {Feature Normalisation for Robust Speech Recognition},
	journal   = {CoRR},
	volume    = {abs/1507.04019},
	year      = {2015},
	url       = {http://arxiv.org/abs/1507.04019},
	archivePrefix = {arXiv},
	eprint    = {1507.04019},
	timestamp = {Wed, 07 Jun 2017 14:41:59 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/Kumar15a},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{practical_cryptography, 
	title={Crypto}, url={http://www.practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/}, journal={Practical Cryptography}
}
@article{lpccvsmfcc,
	abstract = {In this paper two popular feature extraction techniques Linear Predictive Cepstral Coefficients (LPCC) and Mel Frequency Cepstral Coefficients (MFCC) have been investigated and their performances have been evaluated for the recognition of Assamese phonemes. A multilayer perceptron based baseline phoneme recognizer has been built and all the experiments have been carried out using that recognizer. In the present study, attempt has been made to evaluate the performance of the speech recognition system with different feature set in quiet environmental condition as well as at different level of noise. It has been observed that at noise free operating environment when same speaker is used for training and testing the system, the system given 100{\%} recognition accuracy for the recognition of Assamese phones for both the feature set. However, the performance of the system degrades considerably with increase in environmental noise level.It has been observed that the performance of LPCC based system degrades more rapidly compare to MFCC based system under environmental noise condition whereas under speaker variability conditions, LPCC shows relative robustness compare to MFCC though the performance of both the systems degrades considerably.},
	author = {Bhattacharjee, Utpal},
	doi = {10.4236/jis.2012.34041},
	file = {:Users/joshfenech/Documents/Linux Documents Backup May/Documents/Shared Documents/MLDM/Project/Papers/mfccvslpcc.pdf:pdf},
	issn = {2153-1234},
	journal = {International Journal of Engineering Research {\&} Technology},
	keywords = {jit,training,vendor relationship},
	mendeley-groups = {Subtitle Project},
	number = {3},
	pages = {1--6},
	title = {{A comparative study of LPCC and MFCC features for the recognition of Assamese phonemes}},
	url = {https://pdfs.semanticscholar.org/7c8f/8a9d5ba85788b569bc04ca9f07d6ce689e8c.pdf},
	volume = {2},
	year = {2013}
}
@article{Shrawankar2013,
	abstract = {The time domain waveform of a speech signal carries all of the auditory information. From the phonological point of view, it little can be said on the basis of the waveform itself. However, past research in mathematics, acoustics, and speech technology have provided many methods for converting data that can be considered as information if interpreted correctly. In order to find some statistically relevant information from incoming data, it is important to have mechanisms for reducing the information of each segment in the audio signal into a relatively small number of parameters, or features. These features should describe each segment in such a characteristic way that other similar segments can be grouped together by comparing their features. There are enormous interesting and exceptional ways to describe the speech signal in terms of parameters. Though, they all have their strengths and weaknesses, we have presented some of the most used methods with their importance.},
	annote = {Not sure if published in 2010 or 2013.},
	archivePrefix = {arXiv},
	arxivId = {1305.1145},
	author = {Shrawankar, Urmila and Thakare, V M},
	eprint = {1305.1145},
	file = {:Users/joshfenech/Library/Application Support/Mendeley Desktop/Downloaded/Shrawankar, Thakare - 2013 - Techniques for Feature Extraction In Speech Recognition System A Comparative Study.pdf:pdf},
	journal = {International Journal Of Computer Applications In Engineering, Technology and Sciences (IJCAETS),ISSN 0974-3596},
	keywords = {hybrid feature extraction methods,signal processing,speech recognition system},
	mendeley-groups = {Subtitle Project},
	pages = {412--418},
	title = {{Techniques for Feature Extraction In Speech Recognition System : A Comparative Study}},
	url = {http://arxiv.org/abs/1305.1145},
	year = {2013}
}
@misc{catalunya_2017, title={Recurrent Neural Networks II (D2L3 Deep Learning for Speech and Langu...}, url={https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017}, journal={LinkedIn SlideShare}, author={Catalunya, Universitat PolitÃ¨cnica de}, year={2017}}
@article{Sandanalakshmi,
	abstract = {An efficient speech to text converter for mobile application is presented in this work. The prime motive is to formulate a system which would give optimum performance in terms of complexity, accuracy, delay and memory requirements for mobile environment. The speech to text converter consists of two stages namely front-end analysis and patte rn recognition. The front end analysis involves preprocessing and feature extraction. The traditional voice activity detection algorithms which track only energy cannot successfully identify potential speech from input because the unwanted part of the spee ch also has some energy and appears to be speech. In the proposed system , VAD that calculates energy of high frequency part separately as zero crossing rate to differentiate noise from speech is used. Mel Frequency Cepstral Coefficient (MFCC) is used as feature extraction method and Generalized Regression Neural Network is used as recognizer. MFCC provides low word error rate and better feature extraction. Neural Network improves the accuracy. Thus a small database containing all possible syllable pronunciation of the user is sufficient to give recognition accuracy closer to 100{\%}. Thus the proposed technique entertains realization of real time speaker independent applications like mobile phones, PDAs etc.},
	author = {Sandanalakshmi, R and Abinaya, P and Kiruthiga, M and Manjari, M and Sharina, A},
	file = {:Users/joshfenech/Documents/Linux Documents Backup May/Documents/Shared Documents/MLDM/Project/Papers/nyquist.pdf:pdf},
	keywords = {extraction,neural metwork,speech to text converter},
	pages = {1--7},
	title = {{Speaker Independent Continuous Speech to Text Converter for Mobile Application}}
}
@article{Painter2000,
	author = {Painter, T E D and Member, Student and Spanias, Andreas and Member, Senior},
	file = {:Users/joshfenech/Documents/Linux Documents Backup May/Documents/Shared Documents/MLDM/Project/Papers/mp3andotherstuff.pdf:pdf},
	keywords = {aac,ac-2,ac-3,advanced audio coding,atrac,audio coding,audio coding standards,audio signal,broadcast audio,dar,data compression,dba,digital,digital audio radio,filter banks,hdtv,high-definition tv,mpeg,processing},
	number = {4},
	title = {{Perceptual Coding of Digital Audio}},
	volume = {88},
	year = {2000}
}
@misc{fayek_2016, url={http://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html}, journal={Speech Processing for Machine Learning: Filter banks, Mel-Frequency Cepstral Coefficients (MFCCs) and What's In-Between}, publisher={Haytham Fayek}, author={Haytham Fayek}, year={2016}, month={Apr}}